# AI Memory Phase 1 Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Enable AI chat to answer questions about stories by retrieving relevant content from the legacy's story corpus using vector similarity search.

**Architecture:** pgvector extension in PostgreSQL stores embeddings. Stories are chunked and embedded via Amazon Titan v2 in background tasks. Permission-filtered retrieval injects context into AI chat system prompts.

**Tech Stack:** pgvector, Amazon Titan Embeddings v2, SQLAlchemy 2.x, FastAPI BackgroundTasks, pytest

---

## Prerequisites

Before starting, verify:
```bash
# Check PostgreSQL version supports pgvector (15+)
docker compose -f infra/compose/docker-compose.yml exec postgres psql -U postgres -c "SELECT version();"

# Ensure you're in the core-api directory for most commands
cd services/core-api
```

---

## Task 1: Database Migration - Enable pgvector Extension

**Files:**
- Create: `services/core-api/alembic/versions/xxxx_add_pgvector_extension.py`

**Step 1: Generate migration file**

```bash
cd services/core-api
uv run alembic revision -m "add_pgvector_extension"
```

**Step 2: Edit the migration file**

Replace contents of the generated file:

```python
"""add_pgvector_extension

Revision ID: <auto-generated>
Revises: <auto-generated>
Create Date: <auto-generated>

"""

from alembic import op

# revision identifiers, used by Alembic.
revision = "<keep-generated>"
down_revision = "<keep-generated>"
branch_labels = None
depends_on = None


def upgrade() -> None:
    # Enable pgvector extension
    op.execute("CREATE EXTENSION IF NOT EXISTS vector")


def downgrade() -> None:
    # Note: Dropping extension will fail if tables use vector type
    op.execute("DROP EXTENSION IF EXISTS vector")
```

**Step 3: Run migration locally**

```bash
DB_URL="postgresql+psycopg://postgres:postgres@localhost:15432/core" uv run alembic upgrade head
```

Expected: Migration completes without error.

**Step 4: Verify extension is enabled**

```bash
docker compose -f infra/compose/docker-compose.yml exec postgres psql -U postgres -d core -c "SELECT * FROM pg_extension WHERE extname = 'vector';"
```

Expected: One row showing vector extension.

**Step 5: Commit**

```bash
git add alembic/versions/*_add_pgvector_extension.py
git commit -m "chore(db): enable pgvector extension for vector similarity search"
```

---

## Task 2: Database Migration - Create story_chunks Table

**Files:**
- Create: `services/core-api/alembic/versions/xxxx_add_story_chunks_table.py`

**Step 1: Generate migration file**

```bash
cd services/core-api
uv run alembic revision -m "add_story_chunks_table"
```

**Step 2: Edit the migration file**

```python
"""add_story_chunks_table

Revision ID: <auto-generated>
Revises: <auto-generated>
Create Date: <auto-generated>

"""

from alembic import op
import sqlalchemy as sa
from pgvector.sqlalchemy import Vector

# revision identifiers, used by Alembic.
revision = "<keep-generated>"
down_revision = "<keep-generated>"
branch_labels = None
depends_on = None

# Titan v2 embedding dimension
EMBEDDING_DIM = 1024


def upgrade() -> None:
    op.create_table(
        "story_chunks",
        sa.Column("id", sa.UUID(), nullable=False, server_default=sa.text("gen_random_uuid()")),
        sa.Column("story_id", sa.UUID(), nullable=False),
        sa.Column("chunk_index", sa.Integer(), nullable=False),
        sa.Column("content", sa.Text(), nullable=False),
        sa.Column("embedding", Vector(EMBEDDING_DIM), nullable=False),
        sa.Column("legacy_id", sa.UUID(), nullable=False),
        sa.Column("visibility", sa.String(20), nullable=False),
        sa.Column("author_id", sa.UUID(), nullable=False),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("CURRENT_TIMESTAMP"),
            nullable=False,
        ),
        sa.ForeignKeyConstraint(
            ["story_id"],
            ["stories.id"],
            ondelete="CASCADE",
        ),
        sa.ForeignKeyConstraint(
            ["legacy_id"],
            ["legacies.id"],
            ondelete="CASCADE",
        ),
        sa.ForeignKeyConstraint(
            ["author_id"],
            ["users.id"],
            ondelete="CASCADE",
        ),
        sa.PrimaryKeyConstraint("id"),
        sa.UniqueConstraint("story_id", "chunk_index", name="uq_story_chunks_story_id_chunk_index"),
    )

    # Create HNSW index for vector similarity search
    op.execute(
        """
        CREATE INDEX story_chunks_embedding_idx
        ON story_chunks
        USING hnsw (embedding vector_cosine_ops)
        WITH (m = 16, ef_construction = 64)
        """
    )

    # Create indexes for filtering
    op.create_index("ix_story_chunks_legacy_id", "story_chunks", ["legacy_id"])
    op.create_index("ix_story_chunks_legacy_visibility", "story_chunks", ["legacy_id", "visibility"])
    op.create_index("ix_story_chunks_story_id", "story_chunks", ["story_id"])


def downgrade() -> None:
    op.drop_index("ix_story_chunks_story_id", table_name="story_chunks")
    op.drop_index("ix_story_chunks_legacy_visibility", table_name="story_chunks")
    op.drop_index("ix_story_chunks_legacy_id", table_name="story_chunks")
    op.execute("DROP INDEX IF EXISTS story_chunks_embedding_idx")
    op.drop_table("story_chunks")
```

**Step 3: Add pgvector dependency**

```bash
cd services/core-api
uv add pgvector
```

**Step 4: Run migration locally**

```bash
DB_URL="postgresql+psycopg://postgres:postgres@localhost:15432/core" uv run alembic upgrade head
```

Expected: Migration completes without error.

**Step 5: Verify table exists**

```bash
docker compose -f infra/compose/docker-compose.yml exec postgres psql -U postgres -d core -c "\d story_chunks"
```

Expected: Table schema with embedding column of type vector(1024).

**Step 6: Commit**

```bash
git add alembic/versions/*_add_story_chunks_table.py pyproject.toml uv.lock
git commit -m "feat(db): add story_chunks table with pgvector embeddings"
```

---

## Task 3: Database Migration - Create knowledge_audit_log Table

**Files:**
- Create: `services/core-api/alembic/versions/xxxx_add_knowledge_audit_log_table.py`

**Step 1: Generate migration file**

```bash
cd services/core-api
uv run alembic revision -m "add_knowledge_audit_log_table"
```

**Step 2: Edit the migration file**

```python
"""add_knowledge_audit_log_table

Revision ID: <auto-generated>
Revises: <auto-generated>
Create Date: <auto-generated>

"""

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects.postgresql import JSONB

# revision identifiers, used by Alembic.
revision = "<keep-generated>"
down_revision = "<keep-generated>"
branch_labels = None
depends_on = None


def upgrade() -> None:
    op.create_table(
        "knowledge_audit_log",
        sa.Column("id", sa.UUID(), nullable=False, server_default=sa.text("gen_random_uuid()")),
        sa.Column("action", sa.String(50), nullable=False),
        sa.Column("story_id", sa.UUID(), nullable=True),  # NULL if story deleted
        sa.Column("legacy_id", sa.UUID(), nullable=False),
        sa.Column("user_id", sa.UUID(), nullable=False),
        sa.Column("chunk_count", sa.Integer(), nullable=True),
        sa.Column("details", JSONB(), server_default=sa.text("'{}'::jsonb"), nullable=False),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("CURRENT_TIMESTAMP"),
            nullable=False,
        ),
        sa.PrimaryKeyConstraint("id"),
    )

    op.create_index("ix_knowledge_audit_log_legacy_id", "knowledge_audit_log", ["legacy_id"])
    op.create_index("ix_knowledge_audit_log_created_at", "knowledge_audit_log", ["created_at"])
    op.create_index("ix_knowledge_audit_log_story_id", "knowledge_audit_log", ["story_id"])


def downgrade() -> None:
    op.drop_index("ix_knowledge_audit_log_story_id", table_name="knowledge_audit_log")
    op.drop_index("ix_knowledge_audit_log_created_at", table_name="knowledge_audit_log")
    op.drop_index("ix_knowledge_audit_log_legacy_id", table_name="knowledge_audit_log")
    op.drop_table("knowledge_audit_log")
```

**Step 3: Run migration locally**

```bash
DB_URL="postgresql+psycopg://postgres:postgres@localhost:15432/core" uv run alembic upgrade head
```

**Step 4: Verify table exists**

```bash
docker compose -f infra/compose/docker-compose.yml exec postgres psql -U postgres -d core -c "\d knowledge_audit_log"
```

Expected: Table schema displayed.

**Step 5: Commit**

```bash
git add alembic/versions/*_add_knowledge_audit_log_table.py
git commit -m "feat(db): add knowledge_audit_log table for compliance tracking"
```

---

## Task 4: Create SQLAlchemy Models

**Files:**
- Create: `services/core-api/app/models/knowledge.py`
- Modify: `services/core-api/app/models/__init__.py`

**Step 1: Write the test file**

Create `services/core-api/tests/models/test_knowledge.py`:

```python
"""Tests for knowledge models."""

import pytest
from sqlalchemy.ext.asyncio import AsyncSession

from app.models.knowledge import KnowledgeAuditLog, StoryChunk
from app.models.legacy import Legacy
from app.models.story import Story
from app.models.user import User


class TestStoryChunkModel:
    """Tests for StoryChunk model."""

    @pytest.mark.asyncio
    async def test_story_chunk_creates_with_required_fields(
        self,
        db_session: AsyncSession,
        test_user: User,
        test_legacy: Legacy,
        test_story: Story,
    ) -> None:
        """Test StoryChunk can be created with all required fields."""
        # Create a dummy embedding (1024 dimensions)
        embedding = [0.1] * 1024

        chunk = StoryChunk(
            story_id=test_story.id,
            chunk_index=0,
            content="This is the first chunk of content.",
            embedding=embedding,
            legacy_id=test_legacy.id,
            visibility="private",
            author_id=test_user.id,
        )

        db_session.add(chunk)
        await db_session.commit()
        await db_session.refresh(chunk)

        assert chunk.id is not None
        assert chunk.story_id == test_story.id
        assert chunk.chunk_index == 0
        assert chunk.content == "This is the first chunk of content."
        assert len(chunk.embedding) == 1024
        assert chunk.visibility == "private"

    @pytest.mark.asyncio
    async def test_story_chunk_cascade_deletes_with_story(
        self,
        db_session: AsyncSession,
        test_user: User,
        test_legacy: Legacy,
        test_story: Story,
    ) -> None:
        """Test StoryChunk is deleted when parent story is deleted."""
        embedding = [0.1] * 1024

        chunk = StoryChunk(
            story_id=test_story.id,
            chunk_index=0,
            content="Content to be deleted.",
            embedding=embedding,
            legacy_id=test_legacy.id,
            visibility="private",
            author_id=test_user.id,
        )
        db_session.add(chunk)
        await db_session.commit()

        chunk_id = chunk.id

        # Delete the story
        await db_session.delete(test_story)
        await db_session.commit()

        # Verify chunk is also deleted
        from sqlalchemy import select

        result = await db_session.execute(
            select(StoryChunk).where(StoryChunk.id == chunk_id)
        )
        assert result.scalar_one_or_none() is None


class TestKnowledgeAuditLogModel:
    """Tests for KnowledgeAuditLog model."""

    @pytest.mark.asyncio
    async def test_audit_log_creates_with_required_fields(
        self,
        db_session: AsyncSession,
        test_user: User,
        test_legacy: Legacy,
        test_story: Story,
    ) -> None:
        """Test KnowledgeAuditLog can be created."""
        log = KnowledgeAuditLog(
            action="story_indexed",
            story_id=test_story.id,
            legacy_id=test_legacy.id,
            user_id=test_user.id,
            chunk_count=3,
            details={"embedding_model": "titan-v2"},
        )

        db_session.add(log)
        await db_session.commit()
        await db_session.refresh(log)

        assert log.id is not None
        assert log.action == "story_indexed"
        assert log.chunk_count == 3
        assert log.details["embedding_model"] == "titan-v2"
        assert log.created_at is not None
```

**Step 2: Run tests to verify they fail**

```bash
cd services/core-api
uv run pytest tests/models/test_knowledge.py -v
```

Expected: FAIL - `ModuleNotFoundError: No module named 'app.models.knowledge'`

**Step 3: Create the models file**

Create `services/core-api/app/models/knowledge.py`:

```python
"""Knowledge-related models for vector storage and audit logging."""

from datetime import datetime
from typing import Any
from uuid import UUID, uuid4

from pgvector.sqlalchemy import Vector
from sqlalchemy import DateTime, ForeignKey, Integer, String, Text
from sqlalchemy.dialects.postgresql import JSONB
from sqlalchemy.dialects.postgresql import UUID as PG_UUID
from sqlalchemy.orm import Mapped, mapped_column
from sqlalchemy.sql import func

from ..database import Base

# Titan v2 embedding dimension
EMBEDDING_DIM = 1024


class StoryChunk(Base):
    """Vector embedding chunk for a story segment."""

    __tablename__ = "story_chunks"

    id: Mapped[UUID] = mapped_column(
        PG_UUID(as_uuid=True),
        primary_key=True,
        default=uuid4,
    )

    story_id: Mapped[UUID] = mapped_column(
        PG_UUID(as_uuid=True),
        ForeignKey("stories.id", ondelete="CASCADE"),
        nullable=False,
        index=True,
    )

    chunk_index: Mapped[int] = mapped_column(
        Integer,
        nullable=False,
    )

    content: Mapped[str] = mapped_column(
        Text,
        nullable=False,
    )

    embedding: Mapped[list[float]] = mapped_column(
        Vector(EMBEDDING_DIM),
        nullable=False,
    )

    # Denormalized for efficient filtering during vector search
    legacy_id: Mapped[UUID] = mapped_column(
        PG_UUID(as_uuid=True),
        ForeignKey("legacies.id", ondelete="CASCADE"),
        nullable=False,
        index=True,
    )

    visibility: Mapped[str] = mapped_column(
        String(20),
        nullable=False,
    )

    author_id: Mapped[UUID] = mapped_column(
        PG_UUID(as_uuid=True),
        ForeignKey("users.id", ondelete="CASCADE"),
        nullable=False,
    )

    created_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True),
        server_default=func.current_timestamp(),
        nullable=False,
    )

    def __repr__(self) -> str:
        return f"<StoryChunk(id={self.id}, story_id={self.story_id}, index={self.chunk_index})>"


class KnowledgeAuditLog(Base):
    """Audit log for knowledge base operations."""

    __tablename__ = "knowledge_audit_log"

    id: Mapped[UUID] = mapped_column(
        PG_UUID(as_uuid=True),
        primary_key=True,
        default=uuid4,
    )

    action: Mapped[str] = mapped_column(
        String(50),
        nullable=False,
    )

    story_id: Mapped[UUID | None] = mapped_column(
        PG_UUID(as_uuid=True),
        nullable=True,  # NULL if story was deleted
    )

    legacy_id: Mapped[UUID] = mapped_column(
        PG_UUID(as_uuid=True),
        nullable=False,
        index=True,
    )

    user_id: Mapped[UUID] = mapped_column(
        PG_UUID(as_uuid=True),
        nullable=False,
    )

    chunk_count: Mapped[int | None] = mapped_column(
        Integer,
        nullable=True,
    )

    details: Mapped[dict[str, Any]] = mapped_column(
        JSONB,
        nullable=False,
        default=dict,
    )

    created_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True),
        server_default=func.current_timestamp(),
        nullable=False,
        index=True,
    )

    def __repr__(self) -> str:
        return f"<KnowledgeAuditLog(id={self.id}, action={self.action}, story_id={self.story_id})>"
```

**Step 4: Update models __init__.py**

Add to `services/core-api/app/models/__init__.py`:

```python
from .knowledge import KnowledgeAuditLog, StoryChunk

__all__ = [
    # ... existing exports ...
    "KnowledgeAuditLog",
    "StoryChunk",
]
```

**Step 5: Run tests to verify they pass**

```bash
cd services/core-api
uv run pytest tests/models/test_knowledge.py -v
```

Expected: All tests PASS.

**Step 6: Run validation**

```bash
just validate-backend
```

Expected: Ruff and MyPy pass.

**Step 7: Commit**

```bash
git add app/models/knowledge.py app/models/__init__.py tests/models/test_knowledge.py
git commit -m "feat(models): add StoryChunk and KnowledgeAuditLog models"
```

---

## Task 5: Add Embedding Method to Bedrock Adapter

**Files:**
- Modify: `services/core-api/app/adapters/bedrock.py`
- Modify: `services/core-api/tests/adapters/test_bedrock.py`

**Step 1: Write the failing test**

Add to `services/core-api/tests/adapters/test_bedrock.py`:

```python
class TestBedrockAdapterEmbeddings:
    """Tests for embedding generation."""

    @pytest.fixture
    def adapter(self) -> BedrockAdapter:
        """Create adapter instance."""
        return BedrockAdapter(region="us-east-1")

    def test_embed_texts_method_exists(self, adapter: BedrockAdapter) -> None:
        """Test embed_texts method is defined."""
        assert hasattr(adapter, "embed_texts")
        assert callable(adapter.embed_texts)

    @pytest.mark.asyncio
    async def test_embed_texts_returns_embeddings(self, adapter: BedrockAdapter) -> None:
        """Test embed_texts returns list of embeddings."""
        # Mock the Bedrock client
        from unittest.mock import AsyncMock, MagicMock, patch
        import json

        mock_response = {
            "body": MagicMock()
        }
        mock_response["body"].read.return_value = json.dumps({
            "embedding": [0.1] * 1024
        }).encode()

        mock_client = AsyncMock()
        mock_client.invoke_model = AsyncMock(return_value=mock_response)

        with patch.object(adapter, "_get_client") as mock_get_client:
            # Create async context manager mock
            mock_cm = AsyncMock()
            mock_cm.__aenter__.return_value = mock_client
            mock_cm.__aexit__.return_value = None
            mock_get_client.return_value = mock_cm

            result = await adapter.embed_texts(["Hello world"])

            assert len(result) == 1
            assert len(result[0]) == 1024
            assert all(isinstance(x, float) for x in result[0])

    @pytest.mark.asyncio
    async def test_embed_texts_batches_multiple_texts(self, adapter: BedrockAdapter) -> None:
        """Test embed_texts handles multiple texts."""
        from unittest.mock import AsyncMock, MagicMock, patch
        import json

        call_count = 0

        async def mock_invoke(*args, **kwargs):
            nonlocal call_count
            call_count += 1
            mock_response = {"body": MagicMock()}
            mock_response["body"].read.return_value = json.dumps({
                "embedding": [0.1 * call_count] * 1024
            }).encode()
            return mock_response

        mock_client = AsyncMock()
        mock_client.invoke_model = mock_invoke

        with patch.object(adapter, "_get_client") as mock_get_client:
            mock_cm = AsyncMock()
            mock_cm.__aenter__.return_value = mock_client
            mock_cm.__aexit__.return_value = None
            mock_get_client.return_value = mock_cm

            result = await adapter.embed_texts(["Text 1", "Text 2", "Text 3"])

            assert len(result) == 3
            assert call_count == 3  # One API call per text
```

**Step 2: Run tests to verify they fail**

```bash
cd services/core-api
uv run pytest tests/adapters/test_bedrock.py::TestBedrockAdapterEmbeddings -v
```

Expected: FAIL - `AttributeError: 'BedrockAdapter' object has no attribute 'embed_texts'`

**Step 3: Implement embed_texts method**

Add to `services/core-api/app/adapters/bedrock.py` in the `BedrockAdapter` class:

```python
    # Add these constants at the top of the file after imports
    TITAN_EMBED_MODEL_ID = "amazon.titan-embed-text-v2:0"
    TITAN_EMBED_DIMENSION = 1024

    async def embed_texts(
        self,
        texts: list[str],
        model_id: str = TITAN_EMBED_MODEL_ID,
        dimensions: int = TITAN_EMBED_DIMENSION,
    ) -> list[list[float]]:
        """Generate embeddings for a list of texts using Amazon Titan.

        Args:
            texts: List of texts to embed.
            model_id: Titan embedding model ID.
            dimensions: Embedding dimension (256, 512, or 1024).

        Returns:
            List of embedding vectors.

        Raises:
            BedrockError: If embedding generation fails.
        """
        with tracer.start_as_current_span("ai.bedrock.embed") as span:
            span.set_attribute("model_id", model_id)
            span.set_attribute("text_count", len(texts))
            span.set_attribute("dimensions", dimensions)

            embeddings: list[list[float]] = []

            try:
                async with self._get_client() as client:
                    for text in texts:
                        response = await client.invoke_model(
                            modelId=model_id,
                            body=json.dumps({
                                "inputText": text,
                                "dimensions": dimensions,
                                "normalize": True,
                            }),
                        )

                        result = json.loads(response["body"].read())
                        embeddings.append(result["embedding"])

                logger.info(
                    "bedrock.embed_complete",
                    extra={
                        "model_id": model_id,
                        "text_count": len(texts),
                        "dimensions": dimensions,
                    },
                )

                return embeddings

            except ClientError as e:
                error_code = e.response.get("Error", {}).get("Code", "")
                span.set_attribute("error", True)
                logger.error(
                    "bedrock.embed_error",
                    extra={"error_code": error_code, "text_count": len(texts)},
                )

                if error_code == "ThrottlingException":
                    raise BedrockError("Rate limit exceeded", retryable=True) from e
                raise BedrockError(f"Embedding failed: {error_code}", retryable=False) from e

            except Exception as e:
                span.set_attribute("error", True)
                span.record_exception(e)
                logger.error("bedrock.embed_error", extra={"error": str(e)})
                raise BedrockError("Embedding generation failed", retryable=False) from e
```

**Step 4: Add json import if missing**

Ensure `import json` is at the top of `bedrock.py`.

**Step 5: Run tests to verify they pass**

```bash
cd services/core-api
uv run pytest tests/adapters/test_bedrock.py::TestBedrockAdapterEmbeddings -v
```

Expected: All tests PASS.

**Step 6: Run validation**

```bash
just validate-backend
```

Expected: Ruff and MyPy pass.

**Step 7: Commit**

```bash
git add app/adapters/bedrock.py tests/adapters/test_bedrock.py
git commit -m "feat(bedrock): add embed_texts method for Titan v2 embeddings"
```

---

## Task 6: Create Chunking Service

**Files:**
- Create: `services/core-api/app/services/chunking.py`
- Create: `services/core-api/tests/services/test_chunking.py`

**Step 1: Write the failing tests**

Create `services/core-api/tests/services/test_chunking.py`:

```python
"""Tests for chunking service."""

import pytest

from app.services.chunking import chunk_story, estimate_tokens


class TestEstimateTokens:
    """Tests for token estimation."""

    def test_empty_string_returns_zero(self) -> None:
        """Test empty string has zero tokens."""
        assert estimate_tokens("") == 0

    def test_single_word(self) -> None:
        """Test single word token count."""
        # Rough estimate: ~0.75 tokens per word, minimum 1
        result = estimate_tokens("Hello")
        assert result >= 1

    def test_sentence(self) -> None:
        """Test sentence token count."""
        result = estimate_tokens("This is a test sentence with several words.")
        # ~8 words * 1.3 tokens/word ≈ 10 tokens
        assert 5 <= result <= 20


class TestChunkStory:
    """Tests for story chunking."""

    def test_short_story_single_chunk(self) -> None:
        """Test short story becomes single chunk."""
        content = "This is a short story about someone special."
        chunks = chunk_story(content)

        assert len(chunks) == 1
        assert chunks[0] == content

    def test_preserves_paragraph_structure(self) -> None:
        """Test chunking preserves paragraphs when possible."""
        content = """First paragraph with some content.

Second paragraph with more content.

Third paragraph to finish."""

        chunks = chunk_story(content, max_tokens=1000)

        # Should be single chunk since total is small
        assert len(chunks) == 1
        assert "First paragraph" in chunks[0]
        assert "Second paragraph" in chunks[0]
        assert "Third paragraph" in chunks[0]

    def test_splits_long_content(self) -> None:
        """Test long content is split into multiple chunks."""
        # Create content that exceeds max_tokens
        paragraph = "This is a test paragraph with enough words to matter. " * 20
        content = f"{paragraph}\n\n{paragraph}\n\n{paragraph}"

        chunks = chunk_story(content, max_tokens=100)

        assert len(chunks) > 1
        # Each chunk should have content
        for chunk in chunks:
            assert len(chunk.strip()) > 0

    def test_respects_max_tokens(self) -> None:
        """Test no chunk exceeds max_tokens significantly."""
        long_paragraph = "Word " * 500  # ~500 words ≈ 650 tokens
        content = f"{long_paragraph}\n\n{long_paragraph}"

        chunks = chunk_story(content, max_tokens=200)

        for chunk in chunks:
            tokens = estimate_tokens(chunk)
            # Allow some overflow for boundary handling
            assert tokens <= 250, f"Chunk has {tokens} tokens, expected <= 250"

    def test_empty_content_returns_empty_list(self) -> None:
        """Test empty content returns empty list."""
        assert chunk_story("") == []
        assert chunk_story("   ") == []

    def test_handles_single_long_paragraph(self) -> None:
        """Test single paragraph longer than max_tokens is split."""
        long_paragraph = "This is a word. " * 200  # ~800 tokens

        chunks = chunk_story(long_paragraph, max_tokens=100)

        assert len(chunks) > 1
        # Verify overlap exists (chunks share some content)
        # This is implicit in the splitting algorithm

    def test_markdown_headers_preserved(self) -> None:
        """Test markdown structure is preserved in chunks."""
        content = """# Main Title

Introduction paragraph.

## Section One

Content for section one with details.

## Section Two

Content for section two with more details."""

        chunks = chunk_story(content, max_tokens=1000)

        # Single chunk for small content
        assert len(chunks) == 1
        assert "# Main Title" in chunks[0]
        assert "## Section One" in chunks[0]
```

**Step 2: Run tests to verify they fail**

```bash
cd services/core-api
uv run pytest tests/services/test_chunking.py -v
```

Expected: FAIL - `ModuleNotFoundError: No module named 'app.services.chunking'`

**Step 3: Implement the chunking service**

Create `services/core-api/app/services/chunking.py`:

```python
"""Service for chunking story content into embeddable segments."""

import logging
import re

logger = logging.getLogger(__name__)

# Default chunking parameters
DEFAULT_MAX_TOKENS = 500
DEFAULT_OVERLAP_TOKENS = 50
CHARS_PER_TOKEN = 4  # Rough estimate for English text


def estimate_tokens(text: str) -> int:
    """Estimate token count for text.

    Uses character-based estimation. For production accuracy,
    consider using tiktoken or the actual tokenizer.

    Args:
        text: Text to estimate tokens for.

    Returns:
        Estimated token count.
    """
    if not text:
        return 0
    # Rough estimate: ~4 characters per token for English
    return max(1, len(text) // CHARS_PER_TOKEN)


def chunk_story(
    content: str,
    max_tokens: int = DEFAULT_MAX_TOKENS,
    overlap_tokens: int = DEFAULT_OVERLAP_TOKENS,
) -> list[str]:
    """Split story content into chunks for embedding.

    Strategy:
    1. Split by paragraphs (double newline)
    2. Merge small paragraphs until approaching max_tokens
    3. Split oversized paragraphs with overlap

    Args:
        content: Story content (markdown text).
        max_tokens: Maximum tokens per chunk.
        overlap_tokens: Token overlap when splitting large paragraphs.

    Returns:
        List of content chunks.
    """
    if not content or not content.strip():
        return []

    # Split by paragraphs (double newline, preserving markdown structure)
    paragraphs = re.split(r"\n\n+", content.strip())
    paragraphs = [p.strip() for p in paragraphs if p.strip()]

    if not paragraphs:
        return []

    chunks: list[str] = []
    current_chunk = ""

    for paragraph in paragraphs:
        paragraph_tokens = estimate_tokens(paragraph)

        # If single paragraph exceeds max, split it
        if paragraph_tokens > max_tokens:
            # First, save any accumulated content
            if current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = ""

            # Split the large paragraph
            split_chunks = _split_large_paragraph(paragraph, max_tokens, overlap_tokens)
            chunks.extend(split_chunks)
            continue

        # Check if adding this paragraph exceeds max
        combined = f"{current_chunk}\n\n{paragraph}" if current_chunk else paragraph
        combined_tokens = estimate_tokens(combined)

        if combined_tokens <= max_tokens:
            current_chunk = combined
        else:
            # Save current chunk and start new one
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = paragraph

    # Don't forget the last chunk
    if current_chunk:
        chunks.append(current_chunk.strip())

    logger.debug(
        "chunking.complete",
        extra={
            "input_length": len(content),
            "chunk_count": len(chunks),
            "max_tokens": max_tokens,
        },
    )

    return chunks


def _split_large_paragraph(
    paragraph: str,
    max_tokens: int,
    overlap_tokens: int,
) -> list[str]:
    """Split a large paragraph into smaller chunks with overlap.

    Args:
        paragraph: Paragraph text to split.
        max_tokens: Maximum tokens per chunk.
        overlap_tokens: Token overlap between chunks.

    Returns:
        List of paragraph segments.
    """
    # Split by sentences for cleaner breaks
    sentences = re.split(r"(?<=[.!?])\s+", paragraph)

    chunks: list[str] = []
    current_chunk = ""
    overlap_buffer = ""

    for sentence in sentences:
        combined = f"{current_chunk} {sentence}".strip() if current_chunk else sentence
        combined_tokens = estimate_tokens(combined)

        if combined_tokens <= max_tokens:
            current_chunk = combined
        else:
            if current_chunk:
                chunks.append(current_chunk)
                # Keep last part for overlap
                overlap_buffer = _get_overlap_text(current_chunk, overlap_tokens)

            # Start new chunk with overlap
            current_chunk = f"{overlap_buffer} {sentence}".strip() if overlap_buffer else sentence

            # If single sentence is too long, force split by characters
            if estimate_tokens(current_chunk) > max_tokens:
                forced_chunks = _force_split(current_chunk, max_tokens, overlap_tokens)
                chunks.extend(forced_chunks[:-1])
                current_chunk = forced_chunks[-1] if forced_chunks else ""

    if current_chunk:
        chunks.append(current_chunk)

    return chunks


def _get_overlap_text(text: str, overlap_tokens: int) -> str:
    """Get the last N tokens worth of text for overlap.

    Args:
        text: Source text.
        overlap_tokens: Number of tokens to extract.

    Returns:
        Overlap text from end of source.
    """
    overlap_chars = overlap_tokens * CHARS_PER_TOKEN
    if len(text) <= overlap_chars:
        return text
    return text[-overlap_chars:]


def _force_split(text: str, max_tokens: int, overlap_tokens: int) -> list[str]:
    """Force split text by character count when no natural breaks exist.

    Args:
        text: Text to split.
        max_tokens: Maximum tokens per chunk.
        overlap_tokens: Token overlap between chunks.

    Returns:
        List of text segments.
    """
    max_chars = max_tokens * CHARS_PER_TOKEN
    overlap_chars = overlap_tokens * CHARS_PER_TOKEN

    chunks: list[str] = []
    start = 0

    while start < len(text):
        end = min(start + max_chars, len(text))

        # Try to break at word boundary
        if end < len(text):
            space_idx = text.rfind(" ", start, end)
            if space_idx > start:
                end = space_idx

        chunks.append(text[start:end].strip())
        start = end - overlap_chars if end < len(text) else end

    return chunks
```

**Step 4: Run tests to verify they pass**

```bash
cd services/core-api
uv run pytest tests/services/test_chunking.py -v
```

Expected: All tests PASS.

**Step 5: Run validation**

```bash
just validate-backend
```

Expected: Ruff and MyPy pass.

**Step 6: Commit**

```bash
git add app/services/chunking.py tests/services/test_chunking.py
git commit -m "feat(services): add chunking service for story segmentation"
```

---

## Task 7: Create Retrieval Service

**Files:**
- Create: `services/core-api/app/services/retrieval.py`
- Create: `services/core-api/app/schemas/retrieval.py`
- Create: `services/core-api/tests/services/test_retrieval.py`

**Step 1: Create the schemas first**

Create `services/core-api/app/schemas/retrieval.py`:

```python
"""Schemas for knowledge retrieval."""

from uuid import UUID

from pydantic import BaseModel, Field


class ChunkResult(BaseModel):
    """Result from vector similarity search."""

    chunk_id: UUID
    story_id: UUID
    content: str
    similarity: float = Field(..., ge=0.0, le=1.0)


class VisibilityFilter(BaseModel):
    """Filter configuration for visibility-based access."""

    allowed_visibilities: list[str]
    personal_author_id: UUID  # For filtering personal stories to author only
```

**Step 2: Write the failing tests**

Create `services/core-api/tests/services/test_retrieval.py`:

```python
"""Tests for retrieval service."""

from uuid import uuid4

import pytest
from sqlalchemy.ext.asyncio import AsyncSession

from app.models.knowledge import StoryChunk
from app.models.legacy import Legacy, LegacyMember
from app.models.story import Story
from app.models.user import User
from app.schemas.retrieval import VisibilityFilter
from app.services.retrieval import (
    resolve_visibility_filter,
    retrieve_context,
    store_chunks,
    delete_chunks_for_story,
    count_chunks_for_story,
)


class TestResolveVisibilityFilter:
    """Tests for permission resolution."""

    @pytest.mark.asyncio
    async def test_creator_sees_all_visibilities(
        self,
        db_session: AsyncSession,
        test_user: User,
        test_legacy: Legacy,
    ) -> None:
        """Test creator can see all visibility levels."""
        # test_user is creator via fixture
        result = await resolve_visibility_filter(
            db=db_session,
            user_id=test_user.id,
            legacy_id=test_legacy.id,
        )

        assert "public" in result.allowed_visibilities
        assert "private" in result.allowed_visibilities
        assert "personal" in result.allowed_visibilities
        assert result.personal_author_id == test_user.id

    @pytest.mark.asyncio
    async def test_admirer_sees_only_public(
        self,
        db_session: AsyncSession,
        test_legacy: Legacy,
    ) -> None:
        """Test admirer can only see public stories."""
        # Create admirer user
        admirer = User(
            email="admirer@example.com",
            google_id="google_admirer",
            name="Admirer User",
        )
        db_session.add(admirer)
        await db_session.flush()

        # Add as admirer
        membership = LegacyMember(
            legacy_id=test_legacy.id,
            user_id=admirer.id,
            role="admirer",
        )
        db_session.add(membership)
        await db_session.commit()

        result = await resolve_visibility_filter(
            db=db_session,
            user_id=admirer.id,
            legacy_id=test_legacy.id,
        )

        assert result.allowed_visibilities == ["public", "personal"]
        assert result.personal_author_id == admirer.id

    @pytest.mark.asyncio
    async def test_non_member_raises_permission_error(
        self,
        db_session: AsyncSession,
        test_legacy: Legacy,
    ) -> None:
        """Test non-member cannot access legacy."""
        from fastapi import HTTPException

        non_member = User(
            email="nonmember@example.com",
            google_id="google_nonmember",
            name="Non Member",
        )
        db_session.add(non_member)
        await db_session.commit()

        with pytest.raises(HTTPException) as exc:
            await resolve_visibility_filter(
                db=db_session,
                user_id=non_member.id,
                legacy_id=test_legacy.id,
            )

        assert exc.value.status_code == 403


class TestStoreAndDeleteChunks:
    """Tests for chunk storage operations."""

    @pytest.mark.asyncio
    async def test_store_chunks_creates_records(
        self,
        db_session: AsyncSession,
        test_user: User,
        test_legacy: Legacy,
        test_story: Story,
    ) -> None:
        """Test storing chunks creates database records."""
        chunks_data = [
            ("First chunk content", [0.1] * 1024),
            ("Second chunk content", [0.2] * 1024),
        ]

        await store_chunks(
            db=db_session,
            story_id=test_story.id,
            chunks=chunks_data,
            legacy_id=test_legacy.id,
            visibility=test_story.visibility,
            author_id=test_user.id,
        )

        count = await count_chunks_for_story(db_session, test_story.id)
        assert count == 2

    @pytest.mark.asyncio
    async def test_delete_chunks_removes_all(
        self,
        db_session: AsyncSession,
        test_user: User,
        test_legacy: Legacy,
        test_story: Story,
    ) -> None:
        """Test deleting chunks removes all for story."""
        # First store some chunks
        chunks_data = [
            ("Chunk 1", [0.1] * 1024),
            ("Chunk 2", [0.2] * 1024),
        ]
        await store_chunks(
            db=db_session,
            story_id=test_story.id,
            chunks=chunks_data,
            legacy_id=test_legacy.id,
            visibility=test_story.visibility,
            author_id=test_user.id,
        )

        # Verify they exist
        count_before = await count_chunks_for_story(db_session, test_story.id)
        assert count_before == 2

        # Delete them
        deleted = await delete_chunks_for_story(db_session, test_story.id)
        assert deleted == 2

        # Verify they're gone
        count_after = await count_chunks_for_story(db_session, test_story.id)
        assert count_after == 0
```

**Step 3: Run tests to verify they fail**

```bash
cd services/core-api
uv run pytest tests/services/test_retrieval.py -v
```

Expected: FAIL - `ModuleNotFoundError: No module named 'app.services.retrieval'`

**Step 4: Implement the retrieval service**

Create `services/core-api/app/services/retrieval.py`:

```python
"""Service for permission-filtered knowledge retrieval."""

import logging
from uuid import UUID

from fastapi import HTTPException
from opentelemetry import trace
from sqlalchemy import delete, func, select, text
from sqlalchemy.ext.asyncio import AsyncSession

from ..adapters.bedrock import get_bedrock_adapter
from ..models.knowledge import StoryChunk
from ..models.legacy import LegacyMember
from ..schemas.retrieval import ChunkResult, VisibilityFilter

logger = logging.getLogger(__name__)
tracer = trace.get_tracer("core-api.retrieval")

# Roles that can see private content
PRIVATE_ACCESS_ROLES = {"creator", "admin", "advocate"}


async def resolve_visibility_filter(
    db: AsyncSession,
    user_id: UUID,
    legacy_id: UUID,
) -> VisibilityFilter:
    """Determine what visibility levels a user can access.

    Args:
        db: Database session.
        user_id: User requesting access.
        legacy_id: Legacy to check access for.

    Returns:
        VisibilityFilter with allowed visibilities.

    Raises:
        HTTPException: 403 if user is not a member of the legacy.
    """
    with tracer.start_as_current_span("retrieval.resolve_visibility") as span:
        span.set_attribute("user_id", str(user_id))
        span.set_attribute("legacy_id", str(legacy_id))

        # Get user's membership
        result = await db.execute(
            select(LegacyMember).where(
                LegacyMember.legacy_id == legacy_id,
                LegacyMember.user_id == user_id,
                LegacyMember.role != "pending",
            )
        )
        membership = result.scalar_one_or_none()

        if not membership:
            logger.warning(
                "retrieval.access_denied",
                extra={"user_id": str(user_id), "legacy_id": str(legacy_id)},
            )
            raise HTTPException(
                status_code=403,
                detail="Not a member of this legacy",
            )

        role = membership.role
        span.set_attribute("role", role)

        # Determine allowed visibilities based on role
        if role in PRIVATE_ACCESS_ROLES:
            allowed = ["public", "private", "personal"]
        else:
            # Admirers can see public + their own personal
            allowed = ["public", "personal"]

        logger.debug(
            "retrieval.visibility_resolved",
            extra={
                "user_id": str(user_id),
                "legacy_id": str(legacy_id),
                "role": role,
                "allowed_visibilities": allowed,
            },
        )

        return VisibilityFilter(
            allowed_visibilities=allowed,
            personal_author_id=user_id,
        )


async def store_chunks(
    db: AsyncSession,
    story_id: UUID,
    chunks: list[tuple[str, list[float]]],
    legacy_id: UUID,
    visibility: str,
    author_id: UUID,
) -> int:
    """Store story chunks with embeddings.

    Args:
        db: Database session.
        story_id: Story the chunks belong to.
        chunks: List of (content, embedding) tuples.
        legacy_id: Legacy the story belongs to.
        visibility: Story visibility level.
        author_id: Story author ID.

    Returns:
        Number of chunks stored.
    """
    with tracer.start_as_current_span("retrieval.store_chunks") as span:
        span.set_attribute("story_id", str(story_id))
        span.set_attribute("chunk_count", len(chunks))

        for index, (content, embedding) in enumerate(chunks):
            chunk = StoryChunk(
                story_id=story_id,
                chunk_index=index,
                content=content,
                embedding=embedding,
                legacy_id=legacy_id,
                visibility=visibility,
                author_id=author_id,
            )
            db.add(chunk)

        await db.commit()

        logger.info(
            "retrieval.chunks_stored",
            extra={
                "story_id": str(story_id),
                "chunk_count": len(chunks),
            },
        )

        return len(chunks)


async def delete_chunks_for_story(db: AsyncSession, story_id: UUID) -> int:
    """Delete all chunks for a story.

    Args:
        db: Database session.
        story_id: Story to delete chunks for.

    Returns:
        Number of chunks deleted.
    """
    with tracer.start_as_current_span("retrieval.delete_chunks") as span:
        span.set_attribute("story_id", str(story_id))

        result = await db.execute(
            delete(StoryChunk).where(StoryChunk.story_id == story_id)
        )
        await db.commit()

        deleted = result.rowcount
        span.set_attribute("deleted_count", deleted)

        logger.info(
            "retrieval.chunks_deleted",
            extra={"story_id": str(story_id), "deleted_count": deleted},
        )

        return deleted


async def count_chunks_for_story(db: AsyncSession, story_id: UUID) -> int:
    """Count chunks for a story.

    Args:
        db: Database session.
        story_id: Story to count chunks for.

    Returns:
        Number of chunks.
    """
    result = await db.execute(
        select(func.count()).select_from(StoryChunk).where(StoryChunk.story_id == story_id)
    )
    return result.scalar() or 0


async def retrieve_context(
    db: AsyncSession,
    query: str,
    legacy_id: UUID,
    user_id: UUID,
    top_k: int = 5,
) -> list[ChunkResult]:
    """Retrieve relevant story chunks with permission filtering.

    Args:
        db: Database session.
        query: User's question to find relevant content for.
        legacy_id: Legacy to search within.
        user_id: User making the request.
        top_k: Maximum number of results.

    Returns:
        List of relevant chunks the user is authorized to see.
    """
    with tracer.start_as_current_span("retrieval.retrieve_context") as span:
        span.set_attribute("legacy_id", str(legacy_id))
        span.set_attribute("user_id", str(user_id))
        span.set_attribute("top_k", top_k)

        # 1. Resolve permissions
        visibility_filter = await resolve_visibility_filter(db, user_id, legacy_id)

        # 2. Embed the query
        bedrock = get_bedrock_adapter()
        [query_embedding] = await bedrock.embed_texts([query])

        span.set_attribute("query_embedded", True)

        # 3. Build and execute vector search query
        # Using raw SQL for pgvector similarity search
        query_sql = text("""
            SELECT
                id,
                story_id,
                content,
                1 - (embedding <=> :query_embedding::vector) AS similarity
            FROM story_chunks
            WHERE
                legacy_id = :legacy_id
                AND (
                    visibility IN :public_visibilities
                    OR (visibility = 'personal' AND author_id = :author_id)
                )
            ORDER BY embedding <=> :query_embedding::vector
            LIMIT :top_k
        """)

        # Filter out 'personal' from public visibilities since it's handled separately
        public_visibilities = tuple(
            v for v in visibility_filter.allowed_visibilities if v != "personal"
        )

        result = await db.execute(
            query_sql,
            {
                "query_embedding": str(query_embedding),
                "legacy_id": str(legacy_id),
                "public_visibilities": public_visibilities,
                "author_id": str(visibility_filter.personal_author_id),
                "top_k": top_k,
            },
        )

        rows = result.fetchall()

        chunks = [
            ChunkResult(
                chunk_id=row.id,
                story_id=row.story_id,
                content=row.content,
                similarity=float(row.similarity),
            )
            for row in rows
        ]

        span.set_attribute("results_count", len(chunks))

        logger.info(
            "retrieval.context_retrieved",
            extra={
                "legacy_id": str(legacy_id),
                "user_id": str(user_id),
                "query_length": len(query),
                "results_count": len(chunks),
            },
        )

        return chunks
```

**Step 5: Run tests to verify they pass**

```bash
cd services/core-api
uv run pytest tests/services/test_retrieval.py -v
```

Expected: All tests PASS (some may need fixtures - see next step).

**Step 6: Add test_story fixture if missing**

If tests fail due to missing `test_story` fixture, add to `tests/conftest.py`:

```python
@pytest_asyncio.fixture
async def test_story(
    db_session: AsyncSession,
    test_user: User,
    test_legacy: Legacy,
) -> Story:
    """Create a test story."""
    from app.models.story import Story
    from app.models.associations import StoryLegacy

    story = Story(
        author_id=test_user.id,
        title="Test Story",
        content="This is test story content.",
        visibility="private",
    )
    db_session.add(story)
    await db_session.flush()

    # Create association
    story_legacy = StoryLegacy(
        story_id=story.id,
        legacy_id=test_legacy.id,
        role="primary",
        position=0,
    )
    db_session.add(story_legacy)
    await db_session.commit()
    await db_session.refresh(story)

    return story
```

**Step 7: Run validation**

```bash
just validate-backend
```

Expected: Ruff and MyPy pass.

**Step 8: Commit**

```bash
git add app/services/retrieval.py app/schemas/retrieval.py tests/services/test_retrieval.py tests/conftest.py
git commit -m "feat(services): add retrieval service with permission filtering"
```

---

## Task 8: Create Ingestion Service

**Files:**
- Create: `services/core-api/app/services/ingestion.py`
- Create: `services/core-api/tests/services/test_ingestion.py`

**Step 1: Write the failing tests**

Create `services/core-api/tests/services/test_ingestion.py`:

```python
"""Tests for ingestion service."""

from unittest.mock import AsyncMock, patch
from uuid import uuid4

import pytest
from sqlalchemy.ext.asyncio import AsyncSession

from app.models.legacy import Legacy
from app.models.story import Story
from app.models.user import User
from app.services.ingestion import index_story_chunks
from app.services.retrieval import count_chunks_for_story


class TestIndexStoryChunks:
    """Tests for story indexing."""

    @pytest.mark.asyncio
    async def test_index_story_creates_chunks(
        self,
        db_session: AsyncSession,
        test_user: User,
        test_legacy: Legacy,
        test_story: Story,
    ) -> None:
        """Test indexing a story creates chunks."""
        # Mock the embedding call
        with patch("app.services.ingestion.get_bedrock_adapter") as mock_bedrock:
            mock_adapter = AsyncMock()
            mock_adapter.embed_texts = AsyncMock(
                return_value=[[0.1] * 1024]  # One chunk
            )
            mock_bedrock.return_value = mock_adapter

            await index_story_chunks(
                db=db_session,
                story_id=test_story.id,
                content="Short story content.",
                legacy_id=test_legacy.id,
                visibility=test_story.visibility,
                author_id=test_user.id,
            )

        count = await count_chunks_for_story(db_session, test_story.id)
        assert count == 1

    @pytest.mark.asyncio
    async def test_reindex_replaces_existing_chunks(
        self,
        db_session: AsyncSession,
        test_user: User,
        test_legacy: Legacy,
        test_story: Story,
    ) -> None:
        """Test reindexing deletes old chunks and creates new."""
        with patch("app.services.ingestion.get_bedrock_adapter") as mock_bedrock:
            mock_adapter = AsyncMock()
            mock_adapter.embed_texts = AsyncMock(return_value=[[0.1] * 1024])
            mock_bedrock.return_value = mock_adapter

            # Index first time
            await index_story_chunks(
                db=db_session,
                story_id=test_story.id,
                content="Original content.",
                legacy_id=test_legacy.id,
                visibility=test_story.visibility,
                author_id=test_user.id,
            )

            count_first = await count_chunks_for_story(db_session, test_story.id)
            assert count_first == 1

            # Reindex with longer content (2 chunks)
            mock_adapter.embed_texts = AsyncMock(
                return_value=[[0.1] * 1024, [0.2] * 1024]
            )

            await index_story_chunks(
                db=db_session,
                story_id=test_story.id,
                content="New content paragraph one.\n\nNew content paragraph two.",
                legacy_id=test_legacy.id,
                visibility=test_story.visibility,
                author_id=test_user.id,
            )

        count_second = await count_chunks_for_story(db_session, test_story.id)
        # Should only have new chunks, not old + new
        assert count_second == 2

    @pytest.mark.asyncio
    async def test_empty_content_creates_no_chunks(
        self,
        db_session: AsyncSession,
        test_user: User,
        test_legacy: Legacy,
        test_story: Story,
    ) -> None:
        """Test empty content creates no chunks."""
        with patch("app.services.ingestion.get_bedrock_adapter") as mock_bedrock:
            mock_adapter = AsyncMock()
            mock_bedrock.return_value = mock_adapter

            await index_story_chunks(
                db=db_session,
                story_id=test_story.id,
                content="",
                legacy_id=test_legacy.id,
                visibility=test_story.visibility,
                author_id=test_user.id,
            )

            # embed_texts should not have been called
            mock_adapter.embed_texts.assert_not_called()

        count = await count_chunks_for_story(db_session, test_story.id)
        assert count == 0
```

**Step 2: Run tests to verify they fail**

```bash
cd services/core-api
uv run pytest tests/services/test_ingestion.py -v
```

Expected: FAIL - `ModuleNotFoundError: No module named 'app.services.ingestion'`

**Step 3: Implement the ingestion service**

Create `services/core-api/app/services/ingestion.py`:

```python
"""Service for indexing story content into vector store."""

import logging
from uuid import UUID

from opentelemetry import trace
from sqlalchemy.ext.asyncio import AsyncSession

from ..adapters.bedrock import get_bedrock_adapter
from ..models.knowledge import KnowledgeAuditLog
from .chunking import chunk_story
from .retrieval import delete_chunks_for_story, store_chunks

logger = logging.getLogger(__name__)
tracer = trace.get_tracer("core-api.ingestion")


async def index_story_chunks(
    db: AsyncSession,
    story_id: UUID,
    content: str,
    legacy_id: UUID,
    visibility: str,
    author_id: UUID,
    user_id: UUID | None = None,
) -> int:
    """Index story content by chunking, embedding, and storing.

    This is the main ingestion entry point. It:
    1. Deletes any existing chunks for the story
    2. Chunks the content
    3. Generates embeddings via Bedrock Titan
    4. Stores chunks with embeddings
    5. Logs the operation for audit

    Args:
        db: Database session.
        story_id: Story being indexed.
        content: Story content to index.
        legacy_id: Legacy the story belongs to.
        visibility: Story visibility level.
        author_id: Story author ID.
        user_id: User who triggered indexing (for audit, defaults to author).

    Returns:
        Number of chunks created.
    """
    with tracer.start_as_current_span("ingestion.index_story") as span:
        span.set_attribute("story_id", str(story_id))
        span.set_attribute("content_length", len(content))

        # 1. Delete existing chunks (for reindexing)
        old_count = await delete_chunks_for_story(db, story_id)
        span.set_attribute("old_chunk_count", old_count)

        # 2. Chunk the content
        chunks = chunk_story(content)

        if not chunks:
            logger.info(
                "ingestion.no_content",
                extra={"story_id": str(story_id)},
            )
            return 0

        span.set_attribute("new_chunk_count", len(chunks))

        # 3. Generate embeddings
        bedrock = get_bedrock_adapter()
        embeddings = await bedrock.embed_texts(chunks)

        # 4. Store chunks with embeddings
        chunks_with_embeddings = list(zip(chunks, embeddings))
        chunk_count = await store_chunks(
            db=db,
            story_id=story_id,
            chunks=chunks_with_embeddings,
            legacy_id=legacy_id,
            visibility=visibility,
            author_id=author_id,
        )

        # 5. Create audit log entry
        action = "story_reindexed" if old_count > 0 else "story_indexed"
        audit_log = KnowledgeAuditLog(
            action=action,
            story_id=story_id,
            legacy_id=legacy_id,
            user_id=user_id or author_id,
            chunk_count=chunk_count,
            details={
                "content_length": len(content),
                "old_chunk_count": old_count,
                "embedding_model": "amazon.titan-embed-text-v2:0",
            },
        )
        db.add(audit_log)
        await db.commit()

        logger.info(
            f"ingestion.{action}",
            extra={
                "story_id": str(story_id),
                "chunk_count": chunk_count,
                "old_chunk_count": old_count,
            },
        )

        return chunk_count


async def log_deletion_audit(
    db: AsyncSession,
    story_id: UUID,
    legacy_id: UUID,
    user_id: UUID,
    chunk_count: int,
) -> None:
    """Log a story deletion for audit purposes.

    Args:
        db: Database session.
        story_id: Deleted story ID.
        legacy_id: Legacy the story belonged to.
        user_id: User who deleted the story.
        chunk_count: Number of chunks that were deleted.
    """
    audit_log = KnowledgeAuditLog(
        action="story_deleted",
        story_id=story_id,
        legacy_id=legacy_id,
        user_id=user_id,
        chunk_count=chunk_count,
        details={"deletion_source": "user_request"},
    )
    db.add(audit_log)
    await db.commit()

    logger.info(
        "ingestion.story_deleted_audit",
        extra={
            "story_id": str(story_id),
            "user_id": str(user_id),
            "chunk_count": chunk_count,
        },
    )
```

**Step 4: Run tests to verify they pass**

```bash
cd services/core-api
uv run pytest tests/services/test_ingestion.py -v
```

Expected: All tests PASS.

**Step 5: Run validation**

```bash
just validate-backend
```

Expected: Ruff and MyPy pass.

**Step 6: Commit**

```bash
git add app/services/ingestion.py tests/services/test_ingestion.py
git commit -m "feat(services): add ingestion service for story indexing"
```

---

## Task 9: Hook Ingestion into Story Service

**Files:**
- Modify: `services/core-api/app/services/story.py`
- Modify: `services/core-api/app/routes/story.py`

**Step 1: Identify integration points**

Read the existing story service to find create/update/delete functions:

```bash
cd services/core-api
grep -n "async def create_story\|async def update_story\|async def delete_story" app/services/story.py
```

**Step 2: Add background task integration**

Modify `services/core-api/app/services/story.py`:

Add import at top:
```python
from .ingestion import index_story_chunks
```

Find the `create_story` function and add after the commit:

```python
# After: await db.commit()
# Add background task for indexing (passed from route)
# Note: Background task is added in the route layer, not here
# Return data needed for indexing
```

Actually, the cleaner approach is to trigger indexing from the route layer using FastAPI's BackgroundTasks. Let me show the route modification instead.

**Step 3: Modify story routes to add background indexing**

Modify `services/core-api/app/routes/story.py`:

Add imports:
```python
from fastapi import BackgroundTasks
from ..services.ingestion import index_story_chunks
from ..database import get_db_for_background
```

Modify the create story endpoint signature and add background task:

```python
@router.post(
    "/",
    response_model=StoryResponse,
    status_code=status.HTTP_201_CREATED,
)
async def create_story(
    data: StoryCreate,
    request: Request,
    background_tasks: BackgroundTasks,
    db: AsyncSession = Depends(get_db),
) -> StoryResponse:
    """Create a new story."""
    session = require_auth(request)

    story = await story_service.create_story(
        db=db,
        user_id=session.user_id,
        data=data,
    )

    # Queue background indexing for each associated legacy
    if story.legacies:
        primary_legacy = next(
            (leg for leg in story.legacies if leg.role == "primary"),
            story.legacies[0],
        )

        async def background_index():
            async for bg_db in get_db_for_background():
                await index_story_chunks(
                    db=bg_db,
                    story_id=story.id,
                    content=data.content,
                    legacy_id=primary_legacy.legacy_id,
                    visibility=data.visibility,
                    author_id=session.user_id,
                    user_id=session.user_id,
                )

        background_tasks.add_task(background_index)

    return story
```

**Step 4: Add background database session helper**

Add to `services/core-api/app/database.py`:

```python
async def get_db_for_background() -> AsyncGenerator[AsyncSession, None]:
    """Get database session for background tasks.

    Unlike get_db, this creates a fresh session not tied to a request.
    """
    async with async_session_maker() as session:
        try:
            yield session
        finally:
            await session.close()
```

**Step 5: Similarly modify update and delete endpoints**

For update:
```python
@router.put("/{story_id}")
async def update_story(
    story_id: UUID,
    data: StoryUpdate,
    request: Request,
    background_tasks: BackgroundTasks,
    db: AsyncSession = Depends(get_db),
) -> StoryDetail:
    """Update a story."""
    session = require_auth(request)

    story = await story_service.update_story(
        db=db,
        story_id=story_id,
        user_id=session.user_id,
        data=data,
    )

    # Reindex if content changed
    if data.content is not None:
        primary_legacy = next(
            (leg for leg in story.legacies if leg.role == "primary"),
            story.legacies[0] if story.legacies else None,
        )

        if primary_legacy:
            async def background_reindex():
                async for bg_db in get_db_for_background():
                    await index_story_chunks(
                        db=bg_db,
                        story_id=story.id,
                        content=data.content,
                        legacy_id=primary_legacy.legacy_id,
                        visibility=story.visibility,
                        author_id=story.author_id,
                        user_id=session.user_id,
                    )

            background_tasks.add_task(background_reindex)

    return story
```

**Step 6: Run existing story tests**

```bash
cd services/core-api
uv run pytest tests/test_story_api.py -v -k "create or update"
```

Expected: Existing tests still pass (background task doesn't affect response).

**Step 7: Run validation**

```bash
just validate-backend
```

**Step 8: Commit**

```bash
git add app/services/story.py app/routes/story.py app/database.py
git commit -m "feat(stories): add background indexing on create/update"
```

---

## Task 10: Integrate Retrieval into AI Chat

**Files:**
- Modify: `services/core-api/app/services/ai.py`
- Modify: `services/core-api/tests/services/test_ai_service.py`

**Step 1: Add context retrieval to chat flow**

Modify `services/core-api/app/services/ai.py`:

Add import:
```python
from .retrieval import retrieve_context
```

Find the function that builds the system prompt or sends messages. Add context retrieval before calling Bedrock:

```python
async def get_ai_response(
    db: AsyncSession,
    conversation_id: UUID,
    user_message: str,
    user_id: UUID,
) -> AsyncGenerator[str, None]:
    """Get AI response with RAG context."""

    # Get conversation and primary legacy
    conversation = await get_conversation(db, conversation_id, user_id)
    primary_legacy_id = get_primary_legacy_id(conversation)

    # Retrieve relevant story context
    chunks = []
    if primary_legacy_id:
        try:
            chunks = await retrieve_context(
                db=db,
                query=user_message,
                legacy_id=primary_legacy_id,
                user_id=user_id,
                top_k=5,
            )
        except Exception as e:
            logger.warning(
                "ai.retrieval_failed",
                extra={"error": str(e), "conversation_id": str(conversation_id)},
            )
            # Continue without context rather than failing

    # Format context for system prompt
    context_block = format_story_context(chunks)

    # Build system prompt with context
    system_prompt = build_system_prompt(
        persona_id=conversation.persona_id,
        legacy_name=primary_legacy.name if primary_legacy else None,
        story_context=context_block,
    )

    # ... rest of existing logic
```

**Step 2: Add context formatting helper**

Add to `services/core-api/app/services/ai.py`:

```python
def format_story_context(chunks: list[ChunkResult]) -> str:
    """Format retrieved chunks for the system prompt.

    Args:
        chunks: Retrieved story chunks.

    Returns:
        Formatted context string for system prompt.
    """
    if not chunks:
        return ""

    context_parts = ["\n## Relevant stories about this person:\n"]

    for i, chunk in enumerate(chunks, 1):
        context_parts.append(f"[Story excerpt {i}]\n{chunk.content}\n")

    context_parts.append(
        "\nUse these excerpts to inform your responses. "
        "Reference specific details when relevant. "
        "If the excerpts don't contain relevant information, "
        "say so rather than making things up."
    )

    return "\n".join(context_parts)
```

**Step 3: Modify system prompt builder**

Update the system prompt building to include story context:

```python
def build_system_prompt(
    persona_id: str,
    legacy_name: str | None,
    story_context: str = "",
) -> str:
    """Build complete system prompt with persona and context.

    Args:
        persona_id: Persona identifier.
        legacy_name: Name of the legacy being discussed.
        story_context: Retrieved story context.

    Returns:
        Complete system prompt.
    """
    persona = get_persona(persona_id)

    prompt_parts = [
        persona.base_rules,
        persona.system_prompt.format(legacy_name=legacy_name or "this person"),
    ]

    if story_context:
        prompt_parts.append(story_context)

    return "\n\n".join(prompt_parts)
```

**Step 4: Write integration test**

Add to tests:

```python
@pytest.mark.asyncio
async def test_ai_response_includes_story_context(
    db_session: AsyncSession,
    test_user: User,
    test_legacy: Legacy,
    test_conversation: AIConversation,
):
    """Test AI response retrieves and uses story context."""
    # This is more of an integration test
    # Verify that retrieve_context is called during chat
    pass
```

**Step 5: Run validation**

```bash
just validate-backend
```

**Step 6: Commit**

```bash
git add app/services/ai.py
git commit -m "feat(ai): integrate RAG context retrieval into chat"
```

---

## Task 11: Create Backfill Script

**Files:**
- Create: `services/core-api/scripts/backfill_embeddings.py`

**Step 1: Create the backfill script**

Create `services/core-api/scripts/backfill_embeddings.py`:

```python
#!/usr/bin/env python
"""Backfill script to index existing stories into vector store.

Usage:
    cd services/core-api
    uv run python scripts/backfill_embeddings.py

Options:
    --dry-run    Show what would be indexed without actually indexing
    --limit N    Only process N stories (for testing)
"""

import argparse
import asyncio
import logging
import sys
from uuid import UUID

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession, async_sessionmaker, create_async_engine
from sqlalchemy.orm import selectinload

# Add app to path
sys.path.insert(0, ".")

from app.config import get_settings
from app.models.associations import StoryLegacy
from app.models.knowledge import StoryChunk
from app.models.story import Story
from app.services.ingestion import index_story_chunks
from app.services.retrieval import count_chunks_for_story

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)


async def get_stories_without_chunks(
    db: AsyncSession,
    limit: int | None = None,
) -> list[tuple[Story, UUID]]:
    """Get stories that don't have any chunks yet.

    Returns:
        List of (story, primary_legacy_id) tuples.
    """
    # Subquery to find stories with chunks
    stories_with_chunks = select(StoryChunk.story_id).distinct()

    # Query stories without chunks
    query = (
        select(Story)
        .options(selectinload(Story.legacy_associations))
        .where(Story.id.notin_(stories_with_chunks))
        .order_by(Story.created_at)
    )

    if limit:
        query = query.limit(limit)

    result = await db.execute(query)
    stories = result.scalars().all()

    # Extract primary legacy for each story
    stories_with_legacy: list[tuple[Story, UUID]] = []
    for story in stories:
        primary_assoc = next(
            (a for a in story.legacy_associations if a.role == "primary"),
            story.legacy_associations[0] if story.legacy_associations else None,
        )
        if primary_assoc:
            stories_with_legacy.append((story, primary_assoc.legacy_id))
        else:
            logger.warning(f"Story {story.id} has no legacy associations, skipping")

    return stories_with_legacy


async def backfill_stories(
    dry_run: bool = False,
    limit: int | None = None,
) -> None:
    """Backfill embeddings for existing stories."""
    settings = get_settings()

    engine = create_async_engine(settings.database_url, echo=False)
    async_session = async_sessionmaker(engine, expire_on_commit=False)

    async with async_session() as db:
        stories = await get_stories_without_chunks(db, limit)
        total = len(stories)

        logger.info(f"Found {total} stories without embeddings")

        if dry_run:
            for story, legacy_id in stories:
                logger.info(
                    f"[DRY RUN] Would index: {story.id} - {story.title[:50]}..."
                )
            return

        success = 0
        failed = 0

        for i, (story, legacy_id) in enumerate(stories, 1):
            try:
                logger.info(
                    f"[{i}/{total}] Indexing: {story.id} - {story.title[:50]}..."
                )

                chunk_count = await index_story_chunks(
                    db=db,
                    story_id=story.id,
                    content=story.content,
                    legacy_id=legacy_id,
                    visibility=story.visibility,
                    author_id=story.author_id,
                    user_id=story.author_id,
                )

                logger.info(f"  Created {chunk_count} chunks")
                success += 1

            except Exception as e:
                logger.error(f"  Failed: {e}")
                failed += 1
                continue

        logger.info(f"Backfill complete: {success} succeeded, {failed} failed")

    await engine.dispose()


def main() -> None:
    """Main entry point."""
    parser = argparse.ArgumentParser(description="Backfill story embeddings")
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Show what would be indexed without indexing",
    )
    parser.add_argument(
        "--limit",
        type=int,
        help="Limit number of stories to process",
    )

    args = parser.parse_args()

    asyncio.run(backfill_stories(dry_run=args.dry_run, limit=args.limit))


if __name__ == "__main__":
    main()
```

**Step 2: Make script executable**

```bash
chmod +x services/core-api/scripts/backfill_embeddings.py
```

**Step 3: Test with dry run**

```bash
cd services/core-api
uv run python scripts/backfill_embeddings.py --dry-run --limit 5
```

Expected: Lists stories that would be indexed.

**Step 4: Commit**

```bash
git add scripts/backfill_embeddings.py
git commit -m "feat(scripts): add backfill script for existing stories"
```

---

## Task 12: Final Integration Test

**Files:**
- Create: `services/core-api/tests/integration/test_rag_flow.py`

**Step 1: Write end-to-end integration test**

Create `services/core-api/tests/integration/test_rag_flow.py`:

```python
"""Integration tests for RAG flow."""

from unittest.mock import AsyncMock, patch

import pytest
from sqlalchemy.ext.asyncio import AsyncSession

from app.models.knowledge import StoryChunk
from app.models.legacy import Legacy
from app.models.story import Story
from app.models.user import User
from app.services.ingestion import index_story_chunks
from app.services.retrieval import count_chunks_for_story, retrieve_context


class TestRAGFlow:
    """End-to-end tests for retrieval-augmented generation."""

    @pytest.mark.asyncio
    async def test_full_flow_index_and_retrieve(
        self,
        db_session: AsyncSession,
        test_user: User,
        test_legacy: Legacy,
        test_story: Story,
    ) -> None:
        """Test indexing a story and retrieving relevant chunks."""
        story_content = """
        My grandmother was born in 1935 in a small town.

        She loved to garden and spent hours in her backyard
        growing tomatoes, peppers, and beautiful roses.

        During the 1960s, she worked as a teacher at the local
        elementary school where she taught third grade.
        """

        # Mock embeddings
        with patch("app.services.ingestion.get_bedrock_adapter") as mock_ingest:
            mock_adapter = AsyncMock()
            # Return different embeddings for each chunk
            mock_adapter.embed_texts = AsyncMock(
                return_value=[
                    [0.1] * 1024,  # Birth info
                    [0.2] * 1024,  # Garden info
                    [0.3] * 1024,  # Teacher info
                ]
            )
            mock_ingest.return_value = mock_adapter

            # Index the story
            chunk_count = await index_story_chunks(
                db=db_session,
                story_id=test_story.id,
                content=story_content,
                legacy_id=test_legacy.id,
                visibility="private",
                author_id=test_user.id,
            )

            assert chunk_count >= 1

        # Verify chunks exist
        count = await count_chunks_for_story(db_session, test_story.id)
        assert count == chunk_count

        # Test retrieval with mock
        with patch("app.services.retrieval.get_bedrock_adapter") as mock_retrieve:
            mock_adapter = AsyncMock()
            # Query embedding that should match "teacher" chunk
            mock_adapter.embed_texts = AsyncMock(return_value=[[0.3] * 1024])
            mock_retrieve.return_value = mock_adapter

            chunks = await retrieve_context(
                db=db_session,
                query="What did she do for work?",
                legacy_id=test_legacy.id,
                user_id=test_user.id,
                top_k=2,
            )

            # Should return chunks (actual ranking depends on vector similarity)
            assert len(chunks) >= 1
            # At least one chunk should have content
            assert any(c.content for c in chunks)

    @pytest.mark.asyncio
    async def test_visibility_filtering(
        self,
        db_session: AsyncSession,
        test_user: User,
        test_legacy: Legacy,
    ) -> None:
        """Test that visibility filtering works correctly."""
        # Create a private story
        from app.models.story import Story
        from app.models.associations import StoryLegacy

        private_story = Story(
            author_id=test_user.id,
            title="Private Memory",
            content="This is a private memory.",
            visibility="private",
        )
        db_session.add(private_story)
        await db_session.flush()

        story_legacy = StoryLegacy(
            story_id=private_story.id,
            legacy_id=test_legacy.id,
            role="primary",
            position=0,
        )
        db_session.add(story_legacy)
        await db_session.commit()

        # Index the story
        with patch("app.services.ingestion.get_bedrock_adapter") as mock:
            mock_adapter = AsyncMock()
            mock_adapter.embed_texts = AsyncMock(return_value=[[0.5] * 1024])
            mock.return_value = mock_adapter

            await index_story_chunks(
                db=db_session,
                story_id=private_story.id,
                content=private_story.content,
                legacy_id=test_legacy.id,
                visibility="private",
                author_id=test_user.id,
            )

        # Creator should be able to retrieve
        with patch("app.services.retrieval.get_bedrock_adapter") as mock:
            mock_adapter = AsyncMock()
            mock_adapter.embed_texts = AsyncMock(return_value=[[0.5] * 1024])
            mock.return_value = mock_adapter

            chunks = await retrieve_context(
                db=db_session,
                query="memory",
                legacy_id=test_legacy.id,
                user_id=test_user.id,  # Creator
                top_k=5,
            )

            assert len(chunks) >= 1
```

**Step 2: Run integration tests**

```bash
cd services/core-api
uv run pytest tests/integration/test_rag_flow.py -v
```

Expected: All tests PASS.

**Step 3: Run full test suite**

```bash
cd services/core-api
uv run pytest --tb=short
```

Expected: All tests pass.

**Step 4: Run full validation**

```bash
just validate-backend
```

Expected: Ruff and MyPy pass.

**Step 5: Commit**

```bash
git add tests/integration/test_rag_flow.py
git commit -m "test: add RAG flow integration tests"
```

---

## Verification Checklist

After completing all tasks, verify:

- [ ] pgvector extension enabled in PostgreSQL
- [ ] `story_chunks` table exists with HNSW index
- [ ] `knowledge_audit_log` table exists
- [ ] StoryChunk and KnowledgeAuditLog models work
- [ ] Bedrock adapter has `embed_texts()` method
- [ ] Chunking service splits content appropriately
- [ ] Retrieval service filters by visibility
- [ ] Story create/update triggers background indexing
- [ ] AI chat retrieves and uses story context
- [ ] Backfill script runs successfully
- [ ] All tests pass
- [ ] `just validate-backend` passes

---

## Deployment Steps

1. **Merge PR to develop branch**
2. **Deploy to staging environment**
3. **Run migration:**
   ```bash
   kubectl exec -it <core-api-pod> -- alembic upgrade head
   ```
4. **Run backfill script:**
   ```bash
   kubectl exec -it <core-api-pod> -- python scripts/backfill_embeddings.py
   ```
5. **Verify in staging:**
   - Create a story, verify chunks are created
   - Ask AI about story content, verify context is retrieved
6. **Deploy to production following same steps**
